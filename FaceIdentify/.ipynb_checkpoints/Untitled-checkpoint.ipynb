{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n",
      "WARNING: Logging before flag parsing goes to stderr.\n",
      "W1222 20:22:04.883042   808 deprecation_wrapper.py:119] From c:\\users\\dell\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:74: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
      "\n",
      "W1222 20:22:04.900126   808 deprecation_wrapper.py:119] From c:\\users\\dell\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:517: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
      "\n",
      "W1222 20:22:04.902120   808 deprecation_wrapper.py:119] From c:\\users\\dell\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:4138: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n",
      "\n",
      "W1222 20:22:04.937027   808 deprecation_wrapper.py:119] From c:\\users\\dell\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:174: The name tf.get_default_session is deprecated. Please use tf.compat.v1.get_default_session instead.\n",
      "\n",
      "W1222 20:22:04.937027   808 deprecation_wrapper.py:119] From c:\\users\\dell\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:181: The name tf.ConfigProto is deprecated. Please use tf.compat.v1.ConfigProto instead.\n",
      "\n",
      "W1222 20:22:04.964951   808 deprecation_wrapper.py:119] From c:\\users\\dell\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:1834: The name tf.nn.fused_batch_norm is deprecated. Please use tf.compat.v1.nn.fused_batch_norm instead.\n",
      "\n",
      "W1222 20:22:05.079645   808 deprecation_wrapper.py:119] From c:\\users\\dell\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:3976: The name tf.nn.max_pool is deprecated. Please use tf.nn.max_pool2d instead.\n",
      "\n",
      "W1222 20:22:05.769797   808 deprecation.py:506] From c:\\users\\dell\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:3445: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_1 (Conv2D)            (None, 96, 96, 32)        288       \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_1 (LeakyReLU)    (None, 96, 96, 32)        0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_1 (Batch (None, 96, 96, 32)        128       \n",
      "_________________________________________________________________\n",
      "conv2d_2 (Conv2D)            (None, 96, 96, 32)        9216      \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_2 (LeakyReLU)    (None, 96, 96, 32)        0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_2 (Batch (None, 96, 96, 32)        128       \n",
      "_________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2 (None, 48, 48, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_3 (Conv2D)            (None, 48, 48, 64)        18432     \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_3 (LeakyReLU)    (None, 48, 48, 64)        0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_3 (Batch (None, 48, 48, 64)        256       \n",
      "_________________________________________________________________\n",
      "conv2d_4 (Conv2D)            (None, 48, 48, 64)        36864     \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_4 (LeakyReLU)    (None, 48, 48, 64)        0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_4 (Batch (None, 48, 48, 64)        256       \n",
      "_________________________________________________________________\n",
      "max_pooling2d_2 (MaxPooling2 (None, 24, 24, 64)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_5 (Conv2D)            (None, 24, 24, 96)        55296     \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_5 (LeakyReLU)    (None, 24, 24, 96)        0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_5 (Batch (None, 24, 24, 96)        384       \n",
      "_________________________________________________________________\n",
      "conv2d_6 (Conv2D)            (None, 24, 24, 96)        82944     \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_6 (LeakyReLU)    (None, 24, 24, 96)        0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_6 (Batch (None, 24, 24, 96)        384       \n",
      "_________________________________________________________________\n",
      "max_pooling2d_3 (MaxPooling2 (None, 12, 12, 96)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_7 (Conv2D)            (None, 12, 12, 128)       110592    \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_7 (LeakyReLU)    (None, 12, 12, 128)       0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_7 (Batch (None, 12, 12, 128)       512       \n",
      "_________________________________________________________________\n",
      "conv2d_8 (Conv2D)            (None, 12, 12, 128)       147456    \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_8 (LeakyReLU)    (None, 12, 12, 128)       0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_8 (Batch (None, 12, 12, 128)       512       \n",
      "_________________________________________________________________\n",
      "max_pooling2d_4 (MaxPooling2 (None, 6, 6, 128)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_9 (Conv2D)            (None, 6, 6, 256)         294912    \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_9 (LeakyReLU)    (None, 6, 6, 256)         0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_9 (Batch (None, 6, 6, 256)         1024      \n",
      "_________________________________________________________________\n",
      "conv2d_10 (Conv2D)           (None, 6, 6, 256)         589824    \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_10 (LeakyReLU)   (None, 6, 6, 256)         0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_10 (Batc (None, 6, 6, 256)         1024      \n",
      "_________________________________________________________________\n",
      "max_pooling2d_5 (MaxPooling2 (None, 3, 3, 256)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_11 (Conv2D)           (None, 3, 3, 512)         1179648   \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_11 (LeakyReLU)   (None, 3, 3, 512)         0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_11 (Batc (None, 3, 3, 512)         2048      \n",
      "_________________________________________________________________\n",
      "conv2d_12 (Conv2D)           (None, 3, 3, 512)         2359296   \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_12 (LeakyReLU)   (None, 3, 3, 512)         0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_12 (Batc (None, 3, 3, 512)         2048      \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 4608)              0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 512)               2359808   \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 196)               100548    \n",
      "=================================================================\n",
      "Total params: 7,353,828\n",
      "Trainable params: 7,349,476\n",
      "Non-trainable params: 4,352\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random\n",
    "\n",
    "from keras import Sequential\n",
    "from keras.layers import Conv2D, Dropout, MaxPool2D, Flatten, Dense, Convolution2D, LeakyReLU, BatchNormalization\n",
    "\n",
    "model = Sequential()\n",
    "\n",
    "model.add(Convolution2D(32, (3, 3), padding='same', use_bias=False, input_shape=(96, 96, 1)))\n",
    "model.add(LeakyReLU(alpha=0.1))\n",
    "model.add(BatchNormalization())\n",
    "\n",
    "model.add(Convolution2D(32, (3, 3), padding='same', use_bias=False))\n",
    "model.add(LeakyReLU(alpha=0.1))\n",
    "model.add(BatchNormalization())\n",
    "model.add(MaxPool2D(pool_size=(2, 2)))\n",
    "\n",
    "model.add(Convolution2D(64, (3, 3), padding='same', use_bias=False))\n",
    "model.add(LeakyReLU(alpha=0.1))\n",
    "model.add(BatchNormalization())\n",
    "\n",
    "model.add(Convolution2D(64, (3, 3), padding='same', use_bias=False))\n",
    "model.add(LeakyReLU(alpha=0.1))\n",
    "model.add(BatchNormalization())\n",
    "model.add(MaxPool2D(pool_size=(2, 2)))\n",
    "\n",
    "model.add(Convolution2D(96, (3, 3), padding='same', use_bias=False))\n",
    "model.add(LeakyReLU(alpha=0.1))\n",
    "model.add(BatchNormalization())\n",
    "\n",
    "model.add(Convolution2D(96, (3, 3), padding='same', use_bias=False))\n",
    "model.add(LeakyReLU(alpha=0.1))\n",
    "model.add(BatchNormalization())\n",
    "model.add(MaxPool2D(pool_size=(2, 2)))\n",
    "\n",
    "model.add(Convolution2D(128, (3, 3), padding='same', use_bias=False))\n",
    "model.add(LeakyReLU(alpha=0.1))\n",
    "model.add(BatchNormalization())\n",
    "\n",
    "model.add(Convolution2D(128, (3, 3), padding='same', use_bias=False))\n",
    "model.add(LeakyReLU(alpha=0.1))\n",
    "model.add(BatchNormalization())\n",
    "model.add(MaxPool2D(pool_size=(2, 2)))\n",
    "\n",
    "model.add(Convolution2D(256, (3, 3), padding='same', use_bias=False))\n",
    "model.add(LeakyReLU(alpha=0.1))\n",
    "model.add(BatchNormalization())\n",
    "\n",
    "model.add(Convolution2D(256, (3, 3), padding='same', use_bias=False))\n",
    "model.add(LeakyReLU(alpha=0.1))\n",
    "model.add(BatchNormalization())\n",
    "model.add(MaxPool2D(pool_size=(2, 2)))\n",
    "\n",
    "model.add(Convolution2D(512, (3, 3), padding='same', use_bias=False))\n",
    "model.add(LeakyReLU(alpha=0.1))\n",
    "model.add(BatchNormalization())\n",
    "\n",
    "model.add(Convolution2D(512, (3, 3), padding='same', use_bias=False))\n",
    "model.add(LeakyReLU(alpha=0.1))\n",
    "model.add(BatchNormalization())\n",
    "\n",
    "model.add(Flatten())\n",
    "model.add(Dense(512, activation='relu'))\n",
    "model.add(Dropout(0.1))\n",
    "model.add(Dense(196))\n",
    "model.summary()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[[[1.76854902e-03]\n",
      "   [1.41483922e-03]\n",
      "   [1.58400784e-03]\n",
      "   ...\n",
      "   [2.33756078e-03]\n",
      "   [1.53787059e-03]\n",
      "   [2.47597255e-03]]\n",
      "\n",
      "  [[1.64552157e-03]\n",
      "   [1.66089804e-03]\n",
      "   [1.64552157e-03]\n",
      "   ...\n",
      "   [2.15301961e-03]\n",
      "   [2.26067059e-03]\n",
      "   [1.95309412e-03]]\n",
      "\n",
      "  [[1.90696078e-03]\n",
      "   [1.87620000e-03]\n",
      "   [1.41483922e-03]\n",
      "   ...\n",
      "   [1.53787059e-03]\n",
      "   [1.93771765e-03]\n",
      "   [1.52249020e-03]]\n",
      "\n",
      "  ...\n",
      "\n",
      "  [[0.00000000e+00]\n",
      "   [1.53803922e-05]\n",
      "   [1.53803922e-05]\n",
      "   ...\n",
      "   [2.93733333e-03]\n",
      "   [2.87581569e-03]\n",
      "   [2.84505882e-03]]\n",
      "\n",
      "  [[3.07568627e-05]\n",
      "   [3.07568627e-05]\n",
      "   [2.92196078e-04]\n",
      "   ...\n",
      "   [2.93733333e-03]\n",
      "   [2.89119608e-03]\n",
      "   [2.86043922e-03]]\n",
      "\n",
      "  [[3.07568627e-05]\n",
      "   [2.15301961e-04]\n",
      "   [1.32256863e-03]\n",
      "   ...\n",
      "   [2.90657255e-03]\n",
      "   [2.89119608e-03]\n",
      "   [2.86043922e-03]]]\n",
      "\n",
      "\n",
      " [[[3.99847059e-04]\n",
      "   [3.99847059e-04]\n",
      "   [3.84466667e-04]\n",
      "   ...\n",
      "   [4.61360784e-04]\n",
      "   [7.99694118e-04]\n",
      "   [1.03037255e-03]]\n",
      "\n",
      "  [[4.30603922e-04]\n",
      "   [3.99847059e-04]\n",
      "   [3.99847059e-04]\n",
      "   ...\n",
      "   [2.92196078e-04]\n",
      "   [5.38254902e-04]\n",
      "   [7.68933333e-04]]\n",
      "\n",
      "  [[3.53709804e-04]\n",
      "   [3.22952941e-04]\n",
      "   [3.69090196e-04]\n",
      "   ...\n",
      "   [4.45980392e-04]\n",
      "   [3.69090196e-04]\n",
      "   [4.76741176e-04]]\n",
      "\n",
      "  ...\n",
      "\n",
      "  [[8.61207843e-04]\n",
      "   [1.01499608e-03]\n",
      "   [1.01499608e-03]\n",
      "   ...\n",
      "   [3.22952549e-03]\n",
      "   [3.42945098e-03]\n",
      "   [3.61399608e-03]]\n",
      "\n",
      "  [[8.76584314e-04]\n",
      "   [1.06112941e-03]\n",
      "   [1.06112941e-03]\n",
      "   ...\n",
      "   [3.22952549e-03]\n",
      "   [3.30641961e-03]\n",
      "   [3.46020784e-03]]\n",
      "\n",
      "  [[9.07345098e-04]\n",
      "   [1.07650980e-03]\n",
      "   [1.07650980e-03]\n",
      "   ...\n",
      "   [3.24490588e-03]\n",
      "   [3.27566275e-03]\n",
      "   [3.36793725e-03]]]\n",
      "\n",
      "\n",
      " [[[4.30603922e-04]\n",
      "   [3.53709804e-04]\n",
      "   [3.53709804e-04]\n",
      "   ...\n",
      "   [5.69011765e-04]\n",
      "   [5.99768627e-04]\n",
      "   [5.07498039e-04]]\n",
      "\n",
      "  [[3.38333333e-04]\n",
      "   [3.22952941e-04]\n",
      "   [3.22952941e-04]\n",
      "   ...\n",
      "   [5.99768627e-04]\n",
      "   [6.15149020e-04]\n",
      "   [6.30525490e-04]]\n",
      "\n",
      "  [[2.92196078e-04]\n",
      "   [3.38333333e-04]\n",
      "   [3.07572549e-04]\n",
      "   ...\n",
      "   [4.92117647e-04]\n",
      "   [4.45980392e-04]\n",
      "   [5.38254902e-04]]\n",
      "\n",
      "  ...\n",
      "\n",
      "  [[1.01499608e-03]\n",
      "   [9.22721569e-04]\n",
      "   [6.45905882e-04]\n",
      "   ...\n",
      "   [1.99923137e-03]\n",
      "   [2.02998824e-03]\n",
      "   [2.01460784e-03]]\n",
      "\n",
      "  [[3.84466667e-04]\n",
      "   [5.22874510e-04]\n",
      "   [7.38176471e-04]\n",
      "   ...\n",
      "   [2.06074510e-03]\n",
      "   [2.02998824e-03]\n",
      "   [1.96847451e-03]]\n",
      "\n",
      "  [[3.69090196e-04]\n",
      "   [5.84392157e-04]\n",
      "   [6.76662745e-04]\n",
      "   ...\n",
      "   [2.02998824e-03]\n",
      "   [1.98385098e-03]\n",
      "   [2.02998824e-03]]]\n",
      "\n",
      "\n",
      " ...\n",
      "\n",
      "\n",
      " [[[5.84392157e-04]\n",
      "   [6.30525490e-04]\n",
      "   [6.61282353e-04]\n",
      "   ...\n",
      "   [5.84392157e-04]\n",
      "   [5.84392157e-04]\n",
      "   [5.53631373e-04]]\n",
      "\n",
      "  [[5.69011765e-04]\n",
      "   [5.38254902e-04]\n",
      "   [8.61207843e-04]\n",
      "   ...\n",
      "   [6.30525490e-04]\n",
      "   [6.61282353e-04]\n",
      "   [5.99768627e-04]]\n",
      "\n",
      "  [[5.99768627e-04]\n",
      "   [7.22800000e-04]\n",
      "   [6.92043137e-04]\n",
      "   ...\n",
      "   [5.84392157e-04]\n",
      "   [6.30525490e-04]\n",
      "   [6.30525490e-04]]\n",
      "\n",
      "  ...\n",
      "\n",
      "  [[1.44559608e-03]\n",
      "   [1.29181176e-03]\n",
      "   [1.15340392e-03]\n",
      "   ...\n",
      "   [1.15340392e-03]\n",
      "   [1.09188627e-03]\n",
      "   [8.91964706e-04]]\n",
      "\n",
      "  [[1.19953725e-03]\n",
      "   [1.35332549e-03]\n",
      "   [1.07650980e-03]\n",
      "   ...\n",
      "   [1.07650980e-03]\n",
      "   [9.53478431e-04]\n",
      "   [9.53478431e-04]]\n",
      "\n",
      "  [[1.16878039e-03]\n",
      "   [1.23029412e-03]\n",
      "   [1.07650980e-03]\n",
      "   ...\n",
      "   [8.91964706e-04]\n",
      "   [7.84313725e-04]\n",
      "   [9.22721569e-04]]]\n",
      "\n",
      "\n",
      " [[[3.61399608e-03]\n",
      "   [3.30641961e-03]\n",
      "   [2.89119608e-03]\n",
      "   ...\n",
      "   [1.50711373e-03]\n",
      "   [1.56862745e-03]\n",
      "   [1.64552157e-03]]\n",
      "\n",
      "  [[3.27566275e-03]\n",
      "   [2.81430196e-03]\n",
      "   [2.35294118e-03]\n",
      "   ...\n",
      "   [1.53787059e-03]\n",
      "   [1.64552157e-03]\n",
      "   [1.67627843e-03]]\n",
      "\n",
      "  [[2.72203137e-03]\n",
      "   [2.15301961e-03]\n",
      "   [1.75317255e-03]\n",
      "   ...\n",
      "   [1.43021961e-03]\n",
      "   [1.47635686e-03]\n",
      "   [1.47635686e-03]]\n",
      "\n",
      "  ...\n",
      "\n",
      "  [[2.29142745e-03]\n",
      "   [2.12225882e-03]\n",
      "   [2.15301961e-03]\n",
      "   ...\n",
      "   [3.12187451e-03]\n",
      "   [3.09111765e-03]\n",
      "   [3.01422353e-03]]\n",
      "\n",
      "  [[2.24529020e-03]\n",
      "   [2.06074510e-03]\n",
      "   [2.10688235e-03]\n",
      "   ...\n",
      "   [3.01422353e-03]\n",
      "   [2.90657255e-03]\n",
      "   [2.99884706e-03]]\n",
      "\n",
      "  [[2.30680392e-03]\n",
      "   [2.12225882e-03]\n",
      "   [2.18377647e-03]\n",
      "   ...\n",
      "   [2.99884706e-03]\n",
      "   [2.95270980e-03]\n",
      "   [3.04498431e-03]]]\n",
      "\n",
      "\n",
      " [[[9.38101961e-04]\n",
      "   [9.38101961e-04]\n",
      "   [9.38101961e-04]\n",
      "   ...\n",
      "   [2.96809020e-03]\n",
      "   [3.33717647e-03]\n",
      "   [3.62937255e-03]]\n",
      "\n",
      "  [[9.22721569e-04]\n",
      "   [9.38101961e-04]\n",
      "   [9.53478431e-04]\n",
      "   ...\n",
      "   [2.87581569e-03]\n",
      "   [3.21414902e-03]\n",
      "   [3.36793725e-03]]\n",
      "\n",
      "  [[9.22721569e-04]\n",
      "   [9.22721569e-04]\n",
      "   [9.22721569e-04]\n",
      "   ...\n",
      "   [2.92195294e-03]\n",
      "   [3.10649804e-03]\n",
      "   [3.12187451e-03]]\n",
      "\n",
      "  ...\n",
      "\n",
      "  [[9.68858824e-04]\n",
      "   [9.84235294e-04]\n",
      "   [9.68858824e-04]\n",
      "   ...\n",
      "   [9.38101961e-04]\n",
      "   [1.18416078e-03]\n",
      "   [1.59938431e-03]]\n",
      "\n",
      "  [[9.84235294e-04]\n",
      "   [9.99615686e-04]\n",
      "   [9.99615686e-04]\n",
      "   ...\n",
      "   [5.99768627e-04]\n",
      "   [6.92043137e-04]\n",
      "   [1.16878039e-03]]\n",
      "\n",
      "  [[9.99615686e-04]\n",
      "   [9.84235294e-04]\n",
      "   [9.84235294e-04]\n",
      "   ...\n",
      "   [5.38254902e-04]\n",
      "   [6.45905882e-04]\n",
      "   [5.84392157e-04]]]]\n",
      "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                         label\n",
      "3.307007  230.369019 11.857345 252.120847 16.271739 275.014395 13.583872 298.149851 8.765287  321.015727 7.308619  344.331490 9.648163  367.558182 16.526266 389.876907 25.880068 411.301121 36.087363 432.336765 46.822689 453.107391 57.917162 473.689357 68.353330  494.607217 77.434488  516.152584 87.923435  537.012933 104.131531 553.653107 125.559411 562.597153 157.665456 562.444454 188.908844 554.408332 219.060623 542.878448 248.767143 530.226630 277.901545 516.323471 305.315726 499.318180 329.222521 477.701484 348.409992 451.788921 362.785995 422.918097 372.714659 392.216404 379.604212 360.677501 385.110277 328.861987 390.002773 296.946485 393.955999 264.901835 396.966238 232.754155 399.473877 200.562683 1.170013  200.885986 12.455994 178.403046 25.398987 185.622986 39.883026 192.876984 56.238007 202.675995 56.304993  217.125000 39.869995 210.984009 24.569000 205.830994 12.144043 201.320038 123.253998 196.752991 158.710022 184.967010 197.125000 175.759033 234.293030 177.583008 278.075012 198.653992 233.982971 191.651062 196.726990 193.846924 159.261017 205.588989 123.335999 215.856995 96.942993  254.888000 91.317920  293.337128 87.104264  331.789538 75.982487  364.504378 59.906006  379.265015 78.949049  385.135309 98.778631  384.889391 121.971704 379.526973 145.505341 376.044067 15.906006 262.918030 17.796664 248.646269 28.590775 240.214155 54.387677 243.526797 73.959625  260.830811 52.404083 267.578291 29.570987 270.054240 22.292173 267.406626 153.846985 258.848022 166.958665 244.118923 185.233881 237.252822 210.507880 241.799368 234.856812 250.654236 212.660836 261.982240 188.382313 266.367759 171.004499 263.151332 65.271484 436.051025 69.703587  419.619266 87.304969  418.950019 101.458170 419.810235 119.130651 417.269288 164.732409 420.091068 210.542847 420.841736 176.753543 443.858065 139.443459 460.234622 99.561907  468.016744 83.057043  463.948271 71.172928  452.081165 69.075989 436.458008 85.345366  438.435138 101.707421 439.238678 149.303473 434.224917 196.468353 426.042419 149.239728 434.233146 101.578882 439.240243 85.281381  438.429043 38.322242 253.914769 188.963222  248.534734\n",
      "1.002991  94.764008  1.682070  106.841323 2.396685  118.916578 3.180930  130.987499 4.094807  143.049139 5.382909  155.075746 7.374800  167.004224 10.420252 178.705177 14.865761 189.944478 20.939953 200.391388 28.511601 209.811170 37.344603 218.061050 47.247818  224.989407 58.045115  230.419061 69.520356  234.211122 81.426414  236.285819 93.508020  236.583742 105.397696 235.057629 116.815136 231.434759 127.151929 225.387197 136.145741 217.461426 144.197494 208.567166 151.773961 199.263211 158.507154 189.341686 163.199007 178.330175 165.021534 166.491578 165.259883 154.496097 165.381876 142.497279 166.026075 130.515218 166.802429 118.540415 167.231368 106.549244 167.005466 94.552570  166.397278 82.568115  10.348999 89.119995  22.580017 77.860016  36.056030 75.173996  51.072021 70.841995  64.203979 72.157013  64.507996  81.240997  51.622070 82.470978  36.334045 83.462021  23.320007 85.653000  108.923035 72.289001  122.271973 71.063995  134.906006 69.413010  148.325989 70.873001  162.052063 78.964005  148.577087 78.367020  135.216980 78.678009  123.228027 79.216980  109.487976 79.234009  87.974976  94.014008  88.934861  109.321631 90.026028  124.618569 90.243750  139.918750 69.796997  156.917999 80.080136  155.718214 90.270293  153.886861 100.366553 155.236924 110.525879 155.346008 29.973999 101.065002 38.356818 97.887303  46.996269 95.568987  55.428267 96.346060  62.715210  100.747437 54.801831 102.302178 46.766076 102.927228 38.322991 102.441171 114.901978 95.134995  123.248509 91.528774  132.298478 91.096382  138.295504 92.647444  143.988464 95.114059  138.453316 96.078162  132.870876 96.700697  123.829678 96.599006  56.932983 184.148987 71.822099  179.503789 86.215203  174.683742 92.933146  174.548371 98.791248  172.853122 112.083864 175.323659 125.338989 179.515854 116.821614 188.323154 105.946037 193.847230 93.793357  195.122828 81.151308  192.854177 69.163615  188.162456 63.656006 184.993988 78.269473  181.656173 93.187553  180.108325 107.033559 178.467608 120.846191 180.306091 106.830853 178.468581 92.786053  180.164524 78.068823  181.684992 46.143750 98.268750  131.587500   94.900000\n",
      "1.128006  71.462997  1.330412  84.417601  1.393073  97.373615  1.558448  110.328097 2.530592  123.243593 4.877608  135.976958 8.889697  148.285642 14.274508 160.065175 20.702007 171.308589 28.374569 181.737694 37.280533 191.137347 47.131912 199.544956 57.663184  207.086697 68.671619  213.914893 80.353530  219.457662 93.045217  221.979100 105.951729 222.266933 116.446792 218.751258 125.330820 212.103512 132.634064 203.735474 138.400567 194.225455 144.363462 184.836771 150.999679 175.907482 156.574270 166.297286 160.872802 156.037938 164.858626 145.650492 168.128355 135.024519 169.566646 124.008786 170.027237 112.893085 170.412201 101.778253 169.425294 90.702522  167.991085 79.671801  167.702148 68.552567  30.558014 73.052002  45.363007 58.827988  62.289978 57.962982  78.734985 61.285980  95.322998 67.714996  95.139984  74.699005  78.701996 69.667999  62.258972 66.033981  45.959991 65.039993  125.028015 67.921005  136.661987 62.697983  147.644012 59.192978  158.082977 57.190994  167.294983 66.472000  158.165955 63.427002  147.722992 66.718994  136.632996 70.223984  124.896973 74.048996  110.464996 87.233994  111.342016 105.947147 111.930240 124.661060 114.352096 143.037126 85.532013  145.009995 97.354173  148.612929 109.152192 151.788086 119.430055 147.687823 128.700836 141.598114 47.579010 87.759995  57.343311 81.033954  68.795431 78.574668  78.890046 83.055557  86.397125  91.313065  77.079244 91.944592  67.743263 91.825199  57.504511 90.578158  123.609009 90.091003  133.189526 81.815863  144.989650 77.667259  152.456429 80.228278  156.664612 87.148407  150.909261 89.068756  144.910561 89.899826  134.259436 89.931346  62.709991 163.066498 81.483954  160.847899 100.394507 160.639670 110.457646 162.127687 119.816421 161.253436 128.424990 160.242394 137.094696 160.594727 130.487533 172.750957 120.855553 182.578494 107.799261 186.711810 90.569818  184.136989 75.932609  174.614351 65.342987 164.266998 87.176660  169.972893 109.654426 172.295683 121.959113 169.933164 132.620331 163.143951 121.702794 170.050977 109.090583 172.275987 86.897474  169.932471 71.070659 85.106587  144.982036   83.774850\n",
      "3.989014  45.505005  4.713626  51.254430  5.450604  57.002281  6.212105  62.746929  7.041647  68.482018  8.100679  74.178511  9.565857  79.783426  11.518733 85.237141  13.988854 90.476495  16.997617 95.425630  20.561544 99.990326  24.698076 104.041569 29.402466  107.414694 34.587199  109.989644 40.099283  111.761601 45.806017  112.741608 51.593298  112.927884 57.395843  112.298680 63.051080  110.855242 68.445634  108.627109 73.475009  105.665185 78.047314  102.037283 82.060471  97.799708  85.385289  93.004483  87.872759  87.726211  89.574614  82.141983  90.736458  76.418506  91.585571  70.639575  92.201053  64.830967  92.625486  59.005161  92.903604  53.170441  93.082713  47.331775  93.212097  41.491760  9.893066  35.551086  16.160034 31.978088  21.710022 31.623047  28.088074 31.231995  34.398987 31.705994  34.752014  35.890015  28.270020 35.487976  21.867004 35.286987  16.296021 35.169922  54.742981  31.087036  61.533020  29.583069  68.118042  28.373047  74.257080  29.173035  79.974060  32.591064  74.432007  33.280090  68.293030  32.479980  61.852051  33.950073  55.083008  35.267090  44.825012  46.172974  45.414355  54.079869  46.063148  61.975699  45.600000  69.733333  38.882996  74.356018  42.603560  75.000592  46.307497  75.732712  51.476222  74.630411  56.702942  73.594971  16.608215 46.164856  21.052112 44.066023  25.958612 44.377194  29.412828 45.570778  32.698975  47.179016  29.440271 48.262401  26.071255 48.905730  21.073742 48.473175  57.812012  45.893005  61.106408  43.187878  65.275866  42.396387  68.328884  42.925700  71.272034  43.912964  68.579834  45.923315  65.335247  46.744332  61.546159  46.589012  27.385986 83.115967  34.861020  81.923899  42.316826  80.673378  47.296568  80.919928  52.279885  80.936992  61.777200  80.821524  71.220947  81.966614  66.896651  90.346862  58.638312  94.827375  49.251382  96.136261  40.166762  95.170703  32.180466  90.870273  28.876953 84.132996  38.296364  84.325148  47.713978  84.057473  58.766353  83.632127  69.816711  83.157471  60.890880  91.364543  48.826119  93.055033  37.592127  91.455363  27.833333 45.683333  67.050000    44.383333\n",
      "4.287994  68.026001  4.360081  76.558285  4.747252  85.081106  5.773408  93.548694  7.611932  101.877793 10.158972 110.019217 13.300002 117.950581 17.188334 125.540157 22.127017 132.485490 28.210334 138.456315 34.939791 143.699197 41.973253 148.527962 49.285811  152.922293 56.888758  156.790590 64.808558  159.955599 73.059681  162.102811 81.537402  162.879784 87.967293  161.393477 92.669739  156.794172 96.260860  151.197392 100.114321 145.780327 104.229183 140.556276 108.241113 135.253184 112.059849 129.809193 115.725251 124.260777 119.172205 118.574775 122.183808 112.648451 124.547315 106.436598 126.177712 99.993301  127.061003 93.405746  127.275926 86.761271  127.015372 80.117192  126.501282 73.487152  32.156982 65.194000  42.776978 59.841003  52.239014 59.782013  61.627014 64.714996  72.512024 68.295990  72.109985  72.735992  61.439026 68.451996  51.818970 64.578003  42.544006 64.467987  103.127014 70.166992  109.658997 66.500000  115.593018 63.038025  120.823975 59.540985  125.729980 60.140991  120.622986 63.526978  115.504028 67.147003  109.617004 69.670959  103.231018 72.757996  87.161011  76.834991  88.370753  91.112418  89.252877  105.410329 92.875000  119.168750 72.453979  115.294006 79.590150  117.755731 86.480601  120.824586 92.221927  120.400171 97.247925  117.356018 50.513489 75.657196  55.031515 74.231871  59.679481 73.364869  65.467497 74.662135  70.015015  78.541992  64.126277 79.790242  58.165794 78.976775  54.248735 77.530841  104.226013 76.019012  107.926859 73.222350  112.454537 72.287811  117.006150 72.921761  121.084106 75.071411  117.560456 77.257618  113.496088 78.003954  108.745038 77.567880  49.928955 125.237000 64.525443  123.336866 79.051101  126.600638 84.663056  126.103367 90.599793  124.444387 95.398058  123.771119 100.251587 123.829987 97.105373  131.899550 91.788241  138.673204 83.833969  141.711785 70.966895  139.820649 59.117352  134.448390 52.798950 126.054504 68.723771  126.783890 84.570505  128.067325 91.272176  126.755280 97.969849  125.422791 92.713145  133.390054 84.441716  137.840589 67.375669  135.569315 54.925000 76.475000  107.312500   75.443750\n",
      "...                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        ...\n",
      "3.510986  90.559998  3.241833  98.279968  2.911942  105.997532 2.452133  113.708398 1.910525  121.414044 1.542279  129.129293 1.660558  136.850874 2.558937  144.518519 4.384970  152.019249 7.094954  159.248257 10.627602 166.112781 14.967021 172.497136 20.116826  178.246037 26.080937  183.141077 32.821771  186.890217 40.142436  189.322104 47.768513  190.503848 56.901289  190.517005 65.953887  189.288395 74.810111  187.039938 83.401321  183.926220 91.673825  180.043583 99.555303  175.419256 106.933882 170.029943 113.666349 163.854000 119.576574 156.888866 124.454211 149.167315 128.187624 140.830528 130.905862 132.106917 132.833534 123.173301 134.194935 114.134823 135.196100 105.048726 136.021667 95.944702  11.283020 66.344025  19.057983 56.927032  29.429016 56.073975  38.698975 57.071960  45.999023 58.571991  45.281006  63.994965  37.981018 62.494995  28.734985 61.175995  18.637024 62.692993  68.879028  57.095978  79.153015  56.351013  89.549011  57.359039  100.101990 59.624969  106.018005 70.772003  99.577026  66.812988  88.903015  62.793976  78.948975  62.566010  68.729004  62.568054  52.916016  76.033997  50.432016  86.748969  47.072600  97.148511  46.075000  106.775000 38.848022  113.803009 43.553193  116.696729 47.961320  120.007518 56.340624  119.335876 64.890930  118.002991 17.882996 79.010010  21.962696 74.667257  27.573440 72.865401  35.096015 74.996064  41.405640  79.775421  34.923212 81.491962  28.236849 81.787949  22.956535 80.797892  70.073975  80.734985  75.483922  76.019479  82.397800  74.324517  90.456571  76.293698  97.842468  80.188019  91.485511  83.183032  84.599869  84.466494  77.146965  83.390185  27.693970 139.787476 34.850487  134.454907 43.527995  132.671062 47.277338  133.154289 50.097247  133.742056 65.986286  133.872978 79.091858  141.879883 70.061600  148.294238 59.551791  151.605072 48.478465  151.354718 40.066848  150.106268 32.305076  146.866556 29.550476 140.291992 38.022878  135.992300 47.443165  136.116721 62.412554  137.772386 76.893494  141.737000 62.987433  143.123998 49.188695  144.569679 39.434142  142.137170 28.500000 77.087500  83.125000    78.750000\n",
      "1.217903  162.835999 6.954315  173.911683 12.551757 185.058076 17.857871 196.345549 22.787196 207.802918 28.025797 219.117776 34.705583 229.633798 43.400410 238.532018 54.236508 244.609024 66.312965 247.623334 78.752117 248.328143 91.185131 247.362386 103.596973 247.757172 115.264902 252.132969 127.483908 254.353005 139.921602 253.511843 152.289068 251.912171 162.405762 249.179657 171.580993 244.140519 179.735248 237.532124 188.132387 231.236550 196.768983 225.270804 204.665603 218.370209 211.381241 210.316589 216.640119 201.247202 220.171663 191.376810 222.067166 181.060665 222.711038 170.587415 222.195372 160.109151 220.413474 149.770101 216.510299 140.081710 211.096812 131.119704 209.453674 120.790619 34.160995 116.407990 47.259995 98.346008  64.110992 89.010986  81.642990 85.080933  99.268997 81.685974  100.454987 88.402985  82.826981 91.776978  65.218002 96.825958  48.759003 105.261963 147.169983 73.398010  161.201981 71.447998  175.397003 71.421997  188.489014 73.188934  200.704956 85.052002  189.382965 79.243973  176.239990 78.196945  162.450989 78.506012  148.429001 80.372986  132.406006 115.803986 136.384501 127.572352 140.743535 139.141706 142.725000 148.816667 115.729996 164.580994 129.331880 162.285588 142.733007 164.439670 154.322252 159.698443 166.336700 156.022125 60.486588 129.320618 71.315864 121.555356 83.564838 116.448154 95.668549 116.987347 105.052002 124.803986 95.869463 128.466235 86.195837 130.439986 73.313105 130.659709 150.175995 116.049988 153.525162 104.576624 164.402010 99.694769  174.679022 100.605650 184.520782 103.859741 177.766683 109.387167 169.752094 112.816346 160.119653 115.383102 92.528000 200.015991 114.760759 196.651723 136.538433 191.056064 146.261707 188.742269 155.325155 187.216487 167.803864 185.996261 180.317749 185.189606 170.609112 192.661768 160.032496 198.816809 148.421670 202.648292 129.765062 204.356460 111.060921 202.970264 98.473999 199.703003 122.955771 197.452160 147.095759 192.756542 161.541541 189.738705 176.039948 186.983887 162.085805 192.708930 147.438102 196.245190 123.063966 199.679014 80.819876 123.873292 160.650932  109.077019\n",
      "13.656036 114.344543 17.777789 130.350391 17.190038 146.794235 11.917368 162.445308 6.347747  177.995588 5.379075  194.448507 9.132006  210.511367 16.644727 225.219497 26.105230 238.790174 35.690448 252.275324 44.363090 266.361292 51.932992 281.068829 58.075135  296.422933 61.696450  312.535553 63.161711  329.014696 67.554134  344.878587 79.314652  356.105181 103.923075 363.408787 129.617769 364.587729 154.872824 359.796435 178.246739 349.081058 199.218620 334.139335 218.766569 317.350114 237.652185 299.819224 254.851046 280.652756 268.698698 258.962884 278.448959 235.140637 284.185569 210.044248 286.228387 184.371001 285.888007 158.606674 284.668926 132.864748 283.389134 107.125678 282.101013 81.387024  14.528992 95.349976  22.559998 71.699982  35.941986 73.882965  48.498993 78.064026  59.825012 85.572937  59.960999  96.869995  48.500000 91.700012  36.411957 86.943909  22.868011 83.809021  83.127960  77.541016  113.930969 64.829010  143.437012 60.669037  173.278015 65.032013  200.638977 82.867981  173.820007 75.002991  143.640015 71.915985  113.829956 79.347961  83.786987  94.041992  65.195007  119.437988 57.174996  145.426439 49.050114  171.381923 41.703125  197.560417 49.931000  215.713013 60.762999  217.922700 71.740305  219.175108 94.987419  215.518954 113.374847 200.956177 21.511993 125.296997 24.911120 119.053880 30.698971 115.221159 45.718627 117.340537 59.605286  123.839111 47.647770 131.563455 33.749361 133.279516 27.059142 130.207057 117.019989 123.742004 122.234657 109.371590 135.883710 103.345920 156.851237 105.518712 176.669495 112.935547 159.153320 118.247600 141.367782 122.566422 129.301402 125.039033 61.054993 256.059448 53.210374  240.610558 63.701214  232.004700 79.630649  232.259052 95.409831  230.094496 133.164032 228.033060 170.327454 232.351624 146.103018 259.507974 113.829454 277.253858 78.499599  285.876062 65.691251  281.820612 60.557622  269.662667 64.536987 254.942993 70.056691  243.548568 80.059846  236.301359 123.742755 233.479318 167.430054 232.183899 130.461645 261.327340 83.973028  270.798076 73.410920  263.963940 31.034375 124.657292 131.202083  116.952083\n",
      "1.458008  45.985992  1.306287  51.320526  1.186008  56.655847  1.128384  61.992182  1.162910  67.328676  1.331170  72.662448  1.769492  77.980087  2.662499  83.238967  4.210765  88.341791  6.369690  93.219602  8.918218  97.907608  11.627643 102.505290 14.418628  107.053771 17.526386  111.388141 21.365726  115.075420 26.110625  117.459765 31.363272  118.318041 37.074763  117.929322 42.597702  116.416404 47.740291  113.895525 52.373066  110.525294 56.446613  106.494699 59.967821  101.971822 62.991017  97.100552  65.598805  91.993844  67.883165  86.733862  69.919108  81.372549  71.744104  75.935708  73.388419  70.441413  74.886053  64.905251  76.272733  59.340195  77.584840  53.757043  78.859192  48.165131  7.833008  45.565002  11.998962 39.671997  17.885010 39.839005  24.130920 40.229980  28.914978 42.007004  28.528992  46.158005  23.814941 43.626007  17.568970 43.235001  11.835999 43.462997  41.021912  43.531982  47.271912  42.171982  53.304016  41.539001  60.466003  41.608978  65.419922  47.742996  60.079956  45.752991  53.093994  45.401978  46.914001  46.020996  40.718933  46.787994  35.692993  51.641006  34.853293  59.550757  34.221804  67.473331  33.188192  75.276753  24.054993  73.197006  28.334378  75.876463  33.239653  77.317440  37.848104  76.585973  42.489868  76.040009  15.002014 49.546005  17.210054 48.170707  19.777162 47.935813  23.536981 48.789440  26.536072  51.174072  23.206918 52.332448  19.699599 52.367233  17.129077 51.338409  45.111023  54.389008  47.959721  51.891574  51.550637  50.790090  55.416111  51.780222  58.704468  54.104294  55.345425  56.265830  51.357672  56.362870  48.195978  55.502297  17.141479 82.340500  23.464940  81.677901  29.749379  82.730623  32.873349  83.165157  36.522996  83.399282  43.493190  83.935671  50.253601  85.710602  46.212021  92.179023  39.471695  95.683652  31.868572  96.565366  24.526890  95.247719  20.030951  89.316992  18.098022 83.130005  25.280763  83.674228  32.407512  84.727187  40.857256  84.548256  49.149048  86.060562  41.260587  90.748842  32.112399  91.470110  23.412127  90.097072  20.789668 51.365314  51.564576    53.800738\n",
      "7.703339  51.728165  7.993068  56.889577  6.926178  61.950075  6.346912  67.083034  6.947033  72.217215  8.474475  77.159852  10.599921 81.880183  13.017575 86.459467  15.452920 91.029502  17.692726 95.697982  19.903462 100.380207 22.696365 104.729869 26.450074  108.276027 30.964467  110.791149 35.899684  112.331797 41.039536  112.903886 46.206903  112.638869 52.064368  111.566406 57.799058  109.954972 63.411063  107.957507 68.837963  105.503533 73.968053  102.480798 78.622171  98.770870  82.539151  94.294098  85.557995  89.165514  87.811889  83.653987  89.509183  77.944367  90.841576  72.137687  91.978285  66.289101  93.004687  60.420093  93.946657  54.536916  94.830240  48.644663  95.683990  42.748001  6.029022  35.862991  9.739044  30.428009  14.154999 29.262016  18.600037 28.385002  23.458038 27.875999  23.821014  31.562019  18.914001 31.570000  14.470001 32.447006  10.053009 33.613014  41.727997  21.586006  50.450989  18.312019  57.880005  16.475021  65.218994  17.469017  72.195038  23.181999  65.652008  20.617996  58.178986  19.514015  50.739014  21.228996  41.979004  24.133018  32.695007  40.556999  32.184255  46.214267  30.332999  51.410619  30.341461  54.650879  26.605011  67.296005  29.889867  66.124457  33.209422  65.056136  38.164377  63.949914  43.223907  63.494003  12.595001 49.229996  14.466744 45.948359  17.490161 43.725703  21.900096 42.929504  26.329376  43.774712  22.725856 46.635992  18.323898 48.087508  15.494416 48.829965  46.531006  37.634003  50.197216  34.296159  54.699530  32.275412  59.255184  32.272024  63.331451  34.368538  59.490434  35.991641  55.381997  36.693539  50.950972  37.112093  25.417999 83.109009  26.341566  76.533419  30.959106  71.887078  34.332280  70.991960  37.605322  70.492798  46.196420  70.712169  53.816498  74.880432  49.662054  79.342552  44.060696  81.817017  38.228951  83.731844  33.945650  84.668808  29.579413  84.493338  26.286987 82.552002  30.425528  78.574902  35.670121  76.318428  43.966446  75.211294  52.337280  74.883163  44.107402  75.201700  35.945583  76.255510  30.540203  78.492379  17.780124 45.036025  55.118012    34.780124\n",
      "\n",
      "[7500 rows x 1 columns]\n",
      "(7500, 96, 96, 1)\n",
      "(7500, 1)\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Error when checking target: expected dense_2 to have shape (196,) but got array with shape (1,)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-5-42814c1cdd72>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     30\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my_train\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     31\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcompile\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'adam'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mloss\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'mean_squared_error'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmetrics\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'mae'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 32\u001b[1;33m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m256\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalidation_split\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0.2\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mshuffle\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     33\u001b[0m \u001b[1;31m# 加载测试集\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\dell\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, **kwargs)\u001b[0m\n\u001b[0;32m    950\u001b[0m             \u001b[0msample_weight\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    951\u001b[0m             \u001b[0mclass_weight\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mclass_weight\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 952\u001b[1;33m             batch_size=batch_size)\n\u001b[0m\u001b[0;32m    953\u001b[0m         \u001b[1;31m# Prepare validation data.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    954\u001b[0m         \u001b[0mdo_validation\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mFalse\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\dell\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\keras\\engine\\training.py\u001b[0m in \u001b[0;36m_standardize_user_data\u001b[1;34m(self, x, y, sample_weight, class_weight, check_array_lengths, batch_size)\u001b[0m\n\u001b[0;32m    787\u001b[0m                 \u001b[0mfeed_output_shapes\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    788\u001b[0m                 \u001b[0mcheck_batch_axis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m,\u001b[0m  \u001b[1;31m# Don't enforce the batch size.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 789\u001b[1;33m                 exception_prefix='target')\n\u001b[0m\u001b[0;32m    790\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    791\u001b[0m             \u001b[1;31m# Generate sample-wise weight values given the `sample_weight` and\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\dell\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\keras\\engine\\training_utils.py\u001b[0m in \u001b[0;36mstandardize_input_data\u001b[1;34m(data, names, shapes, check_batch_axis, exception_prefix)\u001b[0m\n\u001b[0;32m    136\u001b[0m                             \u001b[1;34m': expected '\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mnames\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;34m' to have shape '\u001b[0m \u001b[1;33m+\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    137\u001b[0m                             \u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;34m' but got array with shape '\u001b[0m \u001b[1;33m+\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 138\u001b[1;33m                             str(data_shape))\n\u001b[0m\u001b[0;32m    139\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mdata\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    140\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: Error when checking target: expected dense_2 to have shape (196,) but got array with shape (1,)"
     ]
    }
   ],
   "source": [
    "\n",
    "TESTING = \"test.csv\"\n",
    "TRAINING = \"train.csv\"\n",
    "\n",
    "\n",
    "def input_data(test=False):\n",
    "    file_name = TESTING if test else TRAINING\n",
    "    df = pd.read_csv(file_name)\n",
    "    cols = df.columns[:-1]  # 标签列\n",
    "\n",
    "    # dropna()是丢弃有缺失数据的样本，这样最后7000多个样本只剩2140个可用的。\n",
    "    df = df.dropna()\n",
    "    df['image'] = df['image'].apply(lambda img: np.fromstring(img, sep=' ') / 255.0)\n",
    "\n",
    "    X = np.vstack(df['image'])\n",
    "    X = X.reshape((-1, 96, 96, 1))\n",
    "\n",
    "    if test:\n",
    "        y = None\n",
    "    else:\n",
    "        y = df[cols].values / 96.0  # 将y值缩放到[0,1]区间\n",
    "\n",
    "    return X, y\n",
    "\n",
    "\n",
    "X_train, y_train = input_data()\n",
    "print(X_train.shape)\n",
    "print(y_train.shape)\n",
    "model.compile(optimizer='adam', loss='mean_squared_error', metrics=['mae'])\n",
    "model.fit(X_train, y_train, epochs=1, batch_size=256, validation_split=0.2, shuffle=True)\n",
    "# 加载测试集"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test, y_test = input_data(test=True)  # 加载测试集\n",
    "pred = model.predict(X_test)\n",
    "print(pred)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
